y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "/r/nfl HOU vs. BUF sentiment analysis",
x = "Comment Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
library(ggjoy)
hou_buff_sentiment %>%
ggplot() +
geom_joy(aes(
x = Created,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "/r/nfl HOU vs. BUF sentiment analysis",
x = "Comment Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
# Houston vs Buffalo
hou_buff = read.csv("data/hou_buff.csv", stringsAsFactors = FALSE)
# Convert UNIX time to regular time stamp
hou_buff = hou_buff %>%
mutate(Created = floor_date(as_datetime(Created), unit = "minute")) %>%
select(Score, Created, Body) %>%
filter(Created <= min(Created) + hours(6))
hou_buff = as_tibble(hou_buff)
# Calculate sentiment
hou_buff_sentiment = hou_buff %>%
unnest_tokens(word, Body)
word_count = hou_buff_sentiment %>%
count(word, sort = TRUE)
my_stop_words <- tibble(
word = c(
"https",
"fucking",
"fuck",
"shit"
),
lexicon = "reddit"
)
all_stop_words <- stop_words %>%
bind_rows(my_stop_words)
hou_buff_sentiment = hou_buff_sentiment %>%
anti_join(all_stop_words) %>%
inner_join(get_sentiments("nrc"))
hou_buff_risky = risky %>%
filter(game_id == 2020010400) %>%
mutate(time = ymd_hms(paste(game_date, time)))
# Plot sentiment over time, overlay with risky plays
hou_buff_sentiment %>%
ggplot(aes(x=Created, y=sent_total)) +
geom_line()
hou_buff_sentiment %>%
ggplot() +
geom_joy(aes(
x = Created,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "/r/nfl HOU vs. BUF sentiment analysis",
x = "Comment Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
library(textdata)
install.packages("textdata")
library(textdata)
# Houston vs Buffalo
hou_buff = read.csv("data/hou_buff.csv", stringsAsFactors = FALSE)
# Convert UNIX time to regular time stamp
hou_buff = hou_buff %>%
mutate(Created = floor_date(as_datetime(Created), unit = "minute")) %>%
select(Score, Created, Body) %>%
filter(Created <= min(Created) + hours(6))
hou_buff = as_tibble(hou_buff)
# Calculate sentiment
hou_buff_sentiment = hou_buff %>%
unnest_tokens(word, Body)
word_count = hou_buff_sentiment %>%
count(word, sort = TRUE)
my_stop_words <- tibble(
word = c(
"https",
"fucking",
"fuck",
"shit"
),
lexicon = "reddit"
)
all_stop_words <- stop_words %>%
bind_rows(my_stop_words)
hou_buff_sentiment = hou_buff_sentiment %>%
anti_join(all_stop_words) %>%
inner_join(get_sentiments("nrc"))
hou_buff_risky = risky %>%
filter(game_id == 2020010400) %>%
mutate(time = ymd_hms(paste(game_date, time)))
# Plot sentiment over time, overlay with risky plays
hou_buff_sentiment %>%
ggplot(aes(x=Created, y=sent_total)) +
geom_line()
hou_buff_sentiment %>%
ggplot() +
geom_joy(aes(
x = Created,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "/r/nfl HOU vs. BUF sentiment analysis",
x = "Comment Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
hou_buff_sentiment = hou_buff_sentiment %>%
anti_join(all_stop_words) %>%
inner_join(get_sentiments("nrc")) %>%
group_by(sentiment) %>%
tally %>%
arrange(desc(n))
hou_buff_risky = risky %>%
filter(game_id == 2020010400) %>%
mutate(time = ymd_hms(paste(game_date, time)))
# Houston vs Buffalo
hou_buff = read.csv("data/hou_buff.csv", stringsAsFactors = FALSE)
# Convert UNIX time to regular time stamp
hou_buff = hou_buff %>%
mutate(Created = floor_date(as_datetime(Created), unit = "minute")) %>%
select(Score, Created, Body) %>%
filter(Created <= min(Created) + hours(6))
hou_buff = as_tibble(hou_buff)
# Calculate sentiment
hou_buff_sentiment = hou_buff %>%
unnest_tokens(word, Body)
word_count = hou_buff_sentiment %>%
count(word, sort = TRUE)
my_stop_words <- tibble(
word = c(
"https",
"fucking",
"fuck",
"shit"
),
lexicon = "reddit"
)
all_stop_words <- stop_words %>%
bind_rows(my_stop_words)
hou_buff_sentiment = hou_buff_sentiment %>%
anti_join(all_stop_words) %>%
inner_join(get_sentiments("nrc"), by = "word")
View(hou_buff_sentiment)
# Houston vs Buffalo
hou_buff = read.csv("data/hou_buff.csv", stringsAsFactors = FALSE)
# Convert UNIX time to regular time stamp
hou_buff = hou_buff %>%
mutate(Created = as_datetime(Created)) %>%
select(Score, Created, Body) %>%
filter(Created <= min(Created) + hours(6))
hou_buff = as_tibble(hou_buff)
# Calculate sentiment
hou_buff_sentiment = hou_buff %>%
unnest_tokens(word, Body)
word_count = hou_buff_sentiment %>%
count(word, sort = TRUE)
my_stop_words <- tibble(
word = c(
"https",
"fucking",
"fuck",
"shit"
),
lexicon = "reddit"
)
all_stop_words <- stop_words %>%
bind_rows(my_stop_words)
hou_buff_sentiment = hou_buff_sentiment %>%
anti_join(all_stop_words) %>%
inner_join(get_sentiments("nrc"), by = "word")
hou_buff_risky = risky %>%
filter(game_id == 2020010400) %>%
mutate(time = ymd_hms(paste(game_date, time)))
hou_buff_sentiment %>%
ggplot() +
geom_joy(aes(
x = Created,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "/r/nfl HOU vs. BUF sentiment analysis",
x = "Comment Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
# Houston vs Buffalo
hou_buff = read.csv("data/hou_buff.csv", stringsAsFactors = FALSE)
# Convert UNIX time to regular time stamp
hou_buff = hou_buff %>%
mutate(Created = as_datetime(Created)) %>%
select(Score, Created, Body) %>%
filter(Created <= min(Created) + hours(2))
hou_buff = as_tibble(hou_buff)
# Calculate sentiment
hou_buff_sentiment = hou_buff %>%
unnest_tokens(word, Body)
word_count = hou_buff_sentiment %>%
count(word, sort = TRUE)
my_stop_words <- tibble(
word = c(
"https",
"fucking",
"fuck",
"shit"
),
lexicon = "reddit"
)
all_stop_words <- stop_words %>%
bind_rows(my_stop_words)
hou_buff_sentiment = hou_buff_sentiment %>%
anti_join(all_stop_words) %>%
inner_join(get_sentiments("nrc"), by = "word")
hou_buff_risky = risky %>%
filter(game_id == 2020010400) %>%
mutate(time = ymd_hms(paste(game_date, time)))
hou_buff_sentiment %>%
ggplot() +
geom_joy(aes(
x = Created,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "/r/nfl HOU vs. BUF sentiment analysis",
x = "Comment Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
# Houston vs Buffalo
hou_buff = read.csv("data/hou_buff.csv", stringsAsFactors = FALSE)
# Convert UNIX time to regular time stamp
hou_buff = hou_buff %>%
mutate(Created = as_datetime(Created)) %>%
select(Score, Created, Body) %>%
filter(Created <= min(Created) + hours(1))
hou_buff = as_tibble(hou_buff)
# Calculate sentiment
hou_buff_sentiment = hou_buff %>%
unnest_tokens(word, Body)
word_count = hou_buff_sentiment %>%
count(word, sort = TRUE)
my_stop_words <- tibble(
word = c(
"https",
"fucking",
"fuck",
"shit"
),
lexicon = "reddit"
)
all_stop_words <- stop_words %>%
bind_rows(my_stop_words)
hou_buff_sentiment = hou_buff_sentiment %>%
anti_join(all_stop_words) %>%
inner_join(get_sentiments("nrc"), by = "word")
hou_buff_risky = risky %>%
filter(game_id == 2020010400) %>%
mutate(time = ymd_hms(paste(game_date, time)))
hou_buff_sentiment %>%
ggplot() +
geom_joy(aes(
x = Created,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "/r/nfl HOU vs. BUF sentiment analysis",
x = "Comment Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
table
table(hou_buff_sentiment$word)
# Convert UNIX time to regular time stamp
hou_buff = hou_buff %>%
mutate(Created = as_datetime(Created)) %>%
select(Score, Created, Body) %>%
filter(Created <= min(Created) + hours(1))
# Houston vs Buffalo
hou_buff = read.csv("data/hou_buff.csv", stringsAsFactors = FALSE)
# Convert UNIX time to regular time stamp
hou_buff = hou_buff %>%
mutate(Created = as_datetime(Created)) %>%
select(Score, Created, Body) %>%
filter(Created <= min(Created) + hours(1))
hou_buff = as_tibble(hou_buff)
# Calculate sentiment
hou_buff_sentiment = hou_buff %>%
unnest_tokens(bigram, Body, token = "ngrams", n = 3, n_min = 2)
View(hou_buff_sentiment)
# Houston vs Buffalo
hou_buff = read.csv("data/hou_buff.csv", stringsAsFactors = FALSE)
# Convert UNIX time to regular time stamp
hou_buff = hou_buff %>%
mutate(Created = as_datetime(Created)) %>%
select(Score, Created, Body) %>%
filter(Created <= min(Created) + hours(1))
hou_buff = as_tibble(hou_buff)
# Calculate sentiment
hou_buff_sentiment = hou_buff %>%
unnest_tokens(bigram, Body, token = "ngrams", n = 2)
my_stop_words <- tibble(
word = c(
"https",
"fucking",
"fuck",
"shit",
"football",
"nfl",
"league"
),
lexicon = "reddit"
)
all_stop_words <- stop_words %>%
bind_rows(my_stop_words)
hou_buff_sentiment = hou_buff_sentiment %>%
anti_join(all_stop_words) %>%
inner_join(get_sentiments("nrc"), by = "word")
hou_buff_sentiment = hou_buff_sentiment %>%
anti_join(all_stop_words) %>%
inner_join(get_sentiments("nrc"), by = "ngrams")
hou_buff_sentiment = hou_buff_sentiment %>%
anti_join(all_stop_words) %>%
inner_join(get_sentiments("nrc"), by = "bigram")
bigrams_separated <- hou_buff_sentiment %>%
separate(bigram, c("word1", "word2"), sep = " ")
View(bigrams_separated)
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% all_stop_words$word) %>%
filter(!word2 %in% all_stop_words$word)
View(bigrams_filtered)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
View(bigrams_united)
# Houston vs Buffalo
hou_buff = read.csv("data/hou_buff.csv", stringsAsFactors = FALSE)
# Convert UNIX time to regular time stamp
hou_buff = hou_buff %>%
mutate(Created = as_datetime(Created)) %>%
select(Score, Created, Body) %>%
filter(Created <= min(Created) + hours(1))
hou_buff = as_tibble(hou_buff)
# Calculate sentiment
hou_buff_sentiment = hou_buff %>%
unnest_tokens(bigram, Body, token = "ngrams", n = 2)
word_count = hou_buff_sentiment %>%
count(word, sort = TRUE)
my_stop_words <- tibble(
word = c(
"https",
"fucking",
"fuck",
"shit",
"football",
"nfl",
"league"
),
lexicon = "reddit"
)
all_stop_words <- stop_words %>%
bind_rows(my_stop_words)
hou_buff_sentiment = hou_buff_sentiment %>%
anti_join(all_stop_words) %>%
inner_join(get_sentiments("nrc"), by = "word")
hou_buff_risky = risky %>%
filter(game_id == 2020010400) %>%
mutate(time = ymd_hms(paste(game_date, time)))
hou_buff_sentiment %>%
ggplot() +
geom_joy(aes(
x = Created,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "/r/nfl HOU vs. BUF sentiment analysis",
x = "Comment Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
# Houston vs Buffalo
hou_buff = read.csv("data/hou_buff.csv", stringsAsFactors = FALSE)
# Convert UNIX time to regular time stamp
hou_buff = hou_buff %>%
mutate(Created = as_datetime(Created)) %>%
select(Score, Created, Body) %>%
filter(Created <= min(Created) + hours(1))
hou_buff = as_tibble(hou_buff)
# Calculate sentiment
hou_buff_sentiment = hou_buff %>%
unnest_tokens(bigram, Body, token = "ngrams", n = 2)
word_count = hou_buff_sentiment %>%
count(word, sort = TRUE)
my_stop_words <- tibble(
word = c(
"https",
"fucking",
"fuck",
"shit",
"football",
"nfl",
"league"
),
lexicon = "reddit"
)
all_stop_words <- stop_words %>%
bind_rows(my_stop_words)
hou_buff_sentiment = hou_buff_sentiment %>%
anti_join(all_stop_words) %>%
inner_join(get_sentiments("nrc"), by = "word")
hou_buff_risky = risky %>%
filter(game_id == 2020010400) %>%
mutate(time = ymd_hms(paste(game_date, time)))
hou_buff_sentiment %>%
ggplot() +
geom_joy(aes(
x = Created,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "/r/nfl HOU vs. BUF sentiment analysis",
x = "Comment Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
View(hou_buff_sentiment)
# Calculate sentiment
hou_buff_sentiment = hou_buff %>%
unnest_tokens(bigram, Body, token = "ngrams", n = 2)
word_count = hou_buff_sentiment %>%
count(word, sort = TRUE)
my_stop_words <- tibble(
word = c(
"https",
"fucking",
"fuck",
"shit",
"football",
"nfl",
"league"
),
lexicon = "reddit"
)
all_stop_words <- stop_words %>%
bind_rows(my_stop_words)
hou_buff_sentiment = hou_buff_sentiment %>%
anti_join(all_stop_words) %>%
inner_join(get_sentiments("nrc"), by = "word")
# Calculate sentiment
hou_buff_sentiment = hou_buff %>%
unnest_tokens(word, Body)
word_count = hou_buff_sentiment %>%
count(word, sort = TRUE)
my_stop_words <- tibble(
word = c(
"https",
"fucking",
"fuck",
"shit",
"football",
"nfl",
"league"
),
lexicon = "reddit"
)
all_stop_words <- stop_words %>%
bind_rows(my_stop_words)
hou_buff_sentiment = hou_buff_sentiment %>%
anti_join(all_stop_words) %>%
inner_join(get_sentiments("nrc"), by = "word")
hou_buff_risky = risky %>%
filter(game_id == 2020010400) %>%
mutate(time = ymd_hms(paste(game_date, time)))
hou_buff_sentiment %>%
ggplot() +
geom_joy(aes(
x = Created,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "/r/nfl HOU vs. BUF sentiment analysis",
x = "Comment Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
